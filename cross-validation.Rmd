---
title: "Handbook of Personality Dynamics - Sensing Technologies for Personality Assessment - Cross-Validation"
author: Clemens Stachl
output: html_notebook
---

In this short tutorial we will illustrate the potential danger of overfitting during variable selection.
First, we will simulate a data matrix with 100 rows and 1000 variables. 
The simulated data is uncorrelated and follows a Gaussian distribution. 
This procedure ensures that statistical relationships between variables are purely caused by chance.
Consequently, we will randomly select one of the 1000 variables as the "criterion".

```{r}
set.seed(645)
data <- matrix(rnorm(100000), nrow = 100, ncol = 1000)
data <- as.data.frame(data)
colnames(data)[1000] <- "criterion"
```

Once done, we calculate pairwise correlations of all other variables with our “criterion”. 
We will only keep the ten most highly correlated variables for fitting a simple linear regression model. 
Please note that all of those variables are completely random and correlations between them exist only due to chance.

```{r}
target_cors <- cor(data)[-1000,"criterion"]
ten_best_vars <- names(sort(abs(target_cors), 
                            decreasing = TRUE)[1:10])
```

After the alleged variable selection is performed, we fit a linear regression model with the ten selected predictor variables and the randomly chosen criterion variable.
Do achieve this we will use the train function as available in the mlr (Machine Learning in R) package.

```{r}
library(mlr) # metapackage for machine learning in R
task_10 <- makeRegrTask(id = "ten_best", 
                        data = data[,c(ten_best_vars, "criterion")], 
                        target = "criterion")
lm <- train("regr.lm", task = task_10)
pred <- predict(lm, task = task_10)
performance(pred, measures = rsq)
```

Our result shows an positive R2 statistic with a respectable effect size. Please not this is the case although we know that there exists no real relationship between the variables. 
This is the result of overfitting. The model has gotten an unfair advantage as the important variables have been extracted from the complete data set. Therefore, some (10)
variables are correlated with the criteria by chance. 

In the book we have introduce cross-validation as a countermeasure to avoid overfitting of the model.
Now we will try to do better and select the best predictor variables within each separate fold of the cross-validation scheme.
We will select important variables on one part of the data and fit the regression model to the other.

```{r}
rdesc <- makeResampleDesc("CV", iters = 10)

lrn <- makeFilterWrapper(learner = "regr.lm", 
                        fw.method = "linear.correlation", fw.abs = 10)
task_1000 <- makeRegrTask(id = "all_vars", 
                          data = data, target = "criterion")
res_nested <- resample(lrn, task = task_1000, resampling = rdesc,
                       measures = rsq)
res_nested

```

This (now correct) procedure gives us a negative R2 statistic, indicating that our model does not generalize to new samples.

